#### Pre-processing Data in python

The process of converting or mapping data from the initial “raw” form into another format, in order to prepare the data for further analysis.

**Pre-processing methods on data**

- Identify and handle missing values
- Data Formatting
- Data Normalization (Centering/Scaling)
- Data Binning

#### Dealing Missing Values
Missing values occur when no data values is stored for a variable (feature) in an observation.
**How to handle missing data**
- Check with the data collection source.
- Drop the missing values
    - drop the variable
    - Drop the data entry
- Replace the missing values (these are less accurate values)
    - Replacer it with an average (of similar data points).
    - replace it by frequency (highest values).
    - replace it based on other functions.
- Leave it as a missing data
**Dropping missing values**

- dropping whole data entry using `dataframes.dropna()`

```python
# dropping the missing values
df.dropna()
```

`axis = 0` drop the entire **rows**
    
`axis=1` drop the entire **columns**
    
**Dropping full rows and columns**
    
```python
# dropping the rows with null values
df.dropna(axis=0)
    
# dropping the columns with null values
df.dropna(axis=1)
```

 **Dropping null values from rows particular columns**
 
```python
# dropping the row with null values for a particular columns
df.dropna(subset=['price'], axis=0)
    
# dropping the row with null values for a particular columns and make the adjustment in dataframe
df.dropna(subset=['price'], axis=0, inplace=True)
```

**Replace missing values**
- Replacing values with different values `dataframe.replace(missing_value, new_values)`.
```python
# replacing the null values withe mean values on columns
mean = df["normalized-losses"].mean()
df["normalized-losses"].replace(np.nan, mean)
```
#### Null Value Cleaning

```python
# importing Python Package
import pandas as pd
import numpy as np
```

For data cleaning, we can use pandas, numpy packages.

```python
# loading the Dataset
df=pd.read_csv("D:\Studies\Workshops\Datascience Workshop\Studies\Dataset\iris - Missing.csv")for Misisng
```

Missing value can be seen as NaN in Dataframe. we can perform following actions to Missing data.

- Remove it from data from dataset
- Filling the missing data from dataset

#### Identifying the missing data in a python Data frame

```python
# checking for missing value
df.isna()

**output:
Sepal.Length	Sepal.Width	Petal.Length	Petal.Width	Species**
0	False	False	False	False	False
1	False	False	False	False	False
```

```python
# checking for missing value
df.isnull()

**output:
Sepal.Length	Sepal.Width	Petal.Length	Petal.Width	Species**
0	False	False	False	False	False
1	False	False	False	False	False
```

```python
# check whether any column has a missing data any(axis=0)
df.isna().any()
**output:**
Sepal.Length    True
Sepal.Width     True
Petal.Length    True
Petal.Width     True
Species         True
dtype: bool

```

```python
# check whether any rows has a missing data any(axis=1)
df.isna().any(axis=1)

**Output:**
0      False
1      False
2       True
3      False
4      False
```

```python
# check whether any columns without missing data **all(axis=0)**
df.isna().all()

**Output:**
Sepal.Length    False
Sepal.Width     False
Petal.Length    False
Petal.Width     False
Species         False
dtype: bool
```

```python
# check whether any columns without missing data all(axis=1)
df.isna().all(axis=1)

**output:**
0      False
1      False
2       True
3      False
4      False
```

```python
# counting null values in columns sun(axis=0)
df.isnull().sum()

**Output:**
Sepal.Length     5
Sepal.Width      7
Petal.Length     2
Petal.Width     85
Species          5
dtype: int64
```

```python
# counting null values in rows sum(axis=1)
df.isnull().sum(axis=1)

**Output:**
0      0
1      0
2      5
3      0
4      0
```

```python
# counting null values in columns sum(axis=0) as a percentage
df.isnull().sum()/len(df)

**output:**
Sepal.Length    0.033333
Sepal.Width     0.046667
Petal.Length    0.013333
Petal.Width     0.566667
Species         0.033333
dtype: float64
```

```python
# drop the rows with any null values in a check  the no of null values
df1=df.dropna(how="any")
df1.isna().any(axis=1).sum()

**output:**
0
```

```python
# drop the rows with all null values in a check  the no of null values
df2=df.dropna(how="all")
df2.isna().any(axis=1).sum()

**output:**
90
```

```python
# to check the cells without missing data
df.notna()

**output:

Sepal.Length	Sepal.Width	Petal.Length	Petal.Width	Species**
0	True	True	True	True	True
1	True	True	True	True	True
2	False	False	False	False	False
```

```python
# Filling the null value
df1=df.fillna(0)
df1.head()

**Output:**

**Sepal.Length	Sepal.Width	Petal.Length	Petal.Width	Species**
0	5.1	3.5	1.4	0.2	setosa
1	4.9	3.0	1.4	0.2	setosa
2	0.0	0.0	0.0	0.0	0
3	4.6	3.1	1.5	0.2	setosa
4	5.0	3.6	1.4	0.2	setosa
```

#### Filling the null value with data available in column (Forward Filling or Backward Filling)

##### Forward Filling
The method **replaces the NULL values with the value from the previous row**  (or previous column, if the axis parameter is set to 'columns' ).

```python
# To create a forward filling on Method="ffill"
df1=df.fillna(method="ffill")
df1.head()

**output:
Sepal.Length	Sepal.Width	Petal.Length	Petal.Width	Species**
0	5.1	3.5	1.4	0.2	setosa
1	4.9	3.0	1.4	0.2	setosa
2	4.9	3.0	1.4	0.2	setosa
3	4.6	3.1	1.5	0.2	setosa
4	5.0	3.6	1.4	0.2	setosa
```

or

```python
# method= "pad"
df2=df.fillna(method="pad")
df2.head()

**output:
Sepal.Length	Sepal.Width	Petal.Length	Petal.Width	Species**
0	5.1	3.5	1.4	0.2	setosa
1	4.9	3.0	1.4	0.2	setosa
2	4.9	3.0	1.4	0.2	setosa
3	4.6	3.1	1.5	0.2	setosa
4	5.0	3.6	1.4	0.2	setosa
```

##### Backward Filling

The method **replaces the NULL values with the value from the Next row**  (or previous column, if the axis parameter is set to 'columns' ).

```python
# To create a forward filing on Method="bfill"
df3=df.fillna(method="bfill")
df3.head()

**output:
Sepal.Length	Sepal.Width	Petal.Length	Petal.Width	Species**
0	5.1	3.5	1.4	0.2	setosa
1	4.9	3.0	1.4	0.2	setosa
2	4.6	3.1	1.5	0.2	setosa
3	4.6	3.1	1.5	0.2	setosa
4	5.0	3.6	1.4	0.2	setosa
```

##### Interpolation in Data Frame

The interpolate() method replaces the NULL values based on a specified method.

```python
# Getting a subset of data
ndf=df.iloc[:,:3]
ndf.head()

# df.interpolate(method="Linear") will be used for fill the null values in numeric data
df3=ndf.interpolate(method="linear")
df3.head()

**output:
Sepal.Length	Sepal.Width	Petal.Length	Petal.Width	Species**
0	5.10	3.50	1.40
1	4.90	3.00	1.40
2	4.75	3.05	1.45
3	4.60	3.10	1.50
4	5.00	3.60	1.40

```

## **Cleaning the Duplicates in Data Frame**

```python
# loading the dataset
df=pd.read_csv("D:\\\\Studies\\\\Workshops\\\\Datascience Workshop\\\\Studies\\\\Dataset\\\\iris - Duplicate.csv")
df.head()

**output:
Id	Sepal.Length	Sepal.Width	Petal.Length	Petal.Width	Species**
0	1001	5.1	3.5	1.4	0.2	setosa
1	1002	4.9	3.0	1.4	0.2	setosa
2	1002	4.9	3.0	1.4	0.2	setosa
3	1003	4.7	3.2	1.3	0.2	setosa
4	1004	4.6	3.1	1.5	0.2	setosa
```

```python
# to check the Data Duplicates
df.duplicated()
**output:**
0      False
1      False
2       True
3      False
4      False
```

```python
# to find all duplicated values
df.duplicated().sum()

**output:** 
3
```

```python
# droping the duplicates from Dataframe
df.drop_duplicates(inplace=True)
```

#### Cleaning the Outliers in the Dataset

```python
# loading the dataset
df=pd.read_csv("D:\\\\Studies\\\\Workshops\\\\Datascience Workshop\\\\Studies\\\\Dataset\\\\iris - Outliers.csv")
df.head()
```

```python
# creating a boxplot to identify the Outliers
df.iloc[:,:4].boxplot()
```

##### Cleaning the Outliers based on IQR

```python
 # identifying Quantiles
Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
```

```python
# Quantile 1
Q1

**Output:**
Sepal.Length    5.1
Sepal.Width     2.8
Petal.Length    1.6
Petal.Width     0.3
Name: 0.25, dtype: float64

# Quantile 3
Q3

**Output:**
Sepal.Length    6.4
Sepal.Width     3.4
Petal.Length    5.1
Petal.Width     1.8
Name: 0.75, dtype: float64

# Inter Quartile Range
IQR=(Q3-Q1)
IQR

**Output:**
Sepal.Length    1.3
Sepal.Width     0.6
Petal.Length    3.5
Petal.Width     1.5
dtype: float64
```

```python
# Creating a dataframe within the range 1.5 IQR from Q1 and Q3 to filter out Outliers
(df<(Q1-1.5*IQR))| (df>(Q3+1.5*IQR))

**Output:**
**Petal.Length	Petal.Width	Sepal.Length	Sepal.Width	Species**
0	False	False	False	False	False
1	False	False	False	True	False
2	False	False	False	False	False
3	False	False	False	False	False
4	False	False	False	False	False
...	...	...	...	...	...
145	False	False	False	False	False
146	False	False	False	True	False
147	False	False	False	False	False
148	False	False	False	False	False
149	False	False	False	False	False
```

```python
# checking Rows with any outliers
((df<(Q1-1.5*IQR))| (df>(Q3+1.5*IQR))).any(axis=1)

**Output:**
0      False
1       True
2      False
3      False
4      False
       ...  
145    False
146     True
147    False
148    False
149    False
Length: 150, dtype: bool
```

```python
# creating a dataframe with only Outliers
df[((df<(Q1-1.5*IQR))| (df>(Q3+1.5*IQR))).any(axis=1)]

**Output:
Sepal.Length	Sepal.Width	Petal.Length	Petal.Width	Species**
1	4.9	28.60	1.4	0.2	setosa
15	5.7	4.40	1.5	0.4	setosa
141	6.9	25.34	5.1	2.3	virginica
146	6.3	30.00	5.0	1.9	virginica
```

```python
# checking Rows without any outliers
~((df<(Q1-1.5*IQR))| (df>(Q3+1.5*IQR))).any(axis=1)

**Output:**
0       True
1      False
2       True
3       True
4       True
       ...  
145     True
146    False
147     True
148     True
149     True
Length: 150, dtype: bool

```

```python
# creating a dataframe without Outliers
df[~((df<(Q1-1.5*IQR))| (df>(Q3+1.5*IQR))).any(axis=1)]

**Output:
Sepal.Length	Sepal.Width	Petal.Length	Petal.Width	Species**
0	5.1	3.5	1.4	0.2	setosa
2	4.7	3.2	1.3	0.2	setosa
3	4.6	3.1	1.5	0.2	setosa
4	5.0	3.6	1.4	0.2	setosa
5	5.4	3.9	1.7	0.4	setosa
...	...	...	...	...	...
144	6.7	3.3	5.7	2.5	virginica
145	6.7	3.0	5.2	2.3	virginica
147	6.5	3.0	5.2	2.0	virginica
148	6.2	3.4	5.4	2.3	virginica
149	5.9	3.0	5.1	1.8	virginica
146 rows × 5 columns
```

##### Cleaning the Outliers based on Z-Score

```python
# importing Python Packages
import pandas as pd
import numpy as np
from scipy import stats
```

```python
# To get the Z Score for Sepal Width
stats.zscore(df["Sepal.Width"])

**Output:**

0     -0.016903
1      7.101452
2     -0.101982
3     -0.130342
4      0.011457
         ...   
145   -0.158702
146    7.498492
147   -0.158702
148   -0.045263
149   -0.158702
Name: Sepal.Width, Length: 150, dtype: float64
```

```python
# get the absolute values of Zscore 
z=np.abs(stats.zscore(df["Sepal.Width"]))
z

**Output:**
0      0.016903
1      7.101452
2      0.101982
3      0.130342
4      0.011457
         ...   
145    0.158702
146    7.498492
147    0.158702
148    0.045263
149    0.158702
Name: Sepal.Width, Length: 150, dtype: float64
```

```python
# to only the get the array (these are the outliers)
np.where(z>3)[0]
**Output:**
array([  1, 141, 146], dtype=int64)

# Convert the array to list
list(np.where(z>3)[0])
**Output:**
[1, 141, 146]
```

```python
# droping the outliers
df1 = df.drop(list(np.where(z>3)[0]))
len(df1)

**Output:**
147
```

The most suitable method is a the IQR Method

#### Cleaning on column in Dataset

##### Changing the column types

```python
# loading the dataset
df=pd.read_csv("D:\\\\Studies\\\\Workshops\\\\Datascience Workshop\\\\Studies\\\\Dataset\\\\iris - ColumnTypes.csv")
df.head()

**output**:
Sepal.Length	Sepal.Width	Petal.Length	Petal.Width	Species
0	"5.1"	3.5	1.4	0.2	setosa
1	"4.9"	3.0	1.4	0.2	setosa
2	"4.7"	3.2	1.3	0.2	setosa
3	"4.6"	3.1	1.5	0.2	setosa
4	"5"	3.6	1.4	0.2	setosa
```

```python
# info on Data Frame
df.info()

**output**:
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 10 entries, 0 to 9
Data columns (total 5 columns):
 #   Column        Non-Null Count  Dtype  
---  ------        --------------  -----  
 0   Sepal.Length  10 non-null     object 
 1   Sepal.Width   10 non-null     float64
 2   Petal.Length  10 non-null     float64
 3   Petal.Width   10 non-null     float64
 4   Species       10 non-null     object 
dtypes: float64(3), object(2)
memory usage: 528.0+ bytes
```

```python
# creating an array based on
arr = df["Sepal.Length"].values
arr
**output**:
array(['"5.1"', '"4.9"', '"4.7"', '"4.6"', '"5"', '"5.4"', '"4.6"', '"5"',
       '"4.4"', '"4.9"'], dtype=object)
```

```python
# joining all list into a string
s1 = ",".join(list(arr))
s1

**output**:
'"5.1","4.9","4.7","4.6","5","5.4","4.6","5","4.4","4.9"'
```

```python
# Replaced the '"' with "" in the string
s2=s1.replace('"',"")
s2

**output**:
'5.1,4.9,4.7,4.6,5,5.4,4.6,5,4.4,4.9'
```

```python
# converting string as an array split by "," as float value
np.array(s2.split(",")).astype(float)

**output**:
array([5.1, 4.9, 4.7, 4.6, 5. , 5.4, 4.6, 5. , 4.4, 4.9])
```

```python
# converting 
df["Sepal.Length"]=np.array(s2.split(",")).astype(float)
df.head()

**output**:
Sepal.Length	Sepal.Width	Petal.Length	Petal.Width	Species
0	5.1	3.5	1.4	0.2	setosa
1	4.9	3.0	1.4	0.2	setosa
2	4.7	3.2	1.3	0.2	setosa
3	4.6	3.1	1.5	0.2	setosa
4	5.0	3.6	1.4	0.2	setosa
```

#### Splitting the column into multiple Columns

##### Using the Loop Method

```python
df.head()

**Output:
Sepal Width & Length	Petal.Width	Species**
0	3.5,1.4	0.2	setosa
1	3,1.4	0.2	setosa
2	3.2,1.3	0.2	setosa
3	3.1,1.5	0.2	setosa
4	3.6,1.4	0.2	setosa
```

```python
# creating a list with the specific column
L=list(df["Sepal Width & Length"].values)
L

**Output:**
['3.5,1.4',
 '3,1.4',
 '3.2,1.3',
 '3.1,1.5',
 '3.6,1.4',
 '3.9,1.7',
 '3.4,1.4',
 '3.4,1.5',
 '2.9,1.4',
 '3.1,1.5']
```

```python
# Getting the split for before comma
l1 = [s.split(",")[0] for s in L]
l1

**Output:**
['3.5', '3', '3.2', '3.1', '3.6', '3.9', '3.4', '3.4', '2.9', '3.1']
```

```python
# Getting the split for after comma
l2 = [s.split(",")[1] for s in L]
l2
**Output:**
['1.4', '1.4', '1.3', '1.5', '1.4', '1.7', '1.4', '1.5', '1.4', '1.5']
```

```python
# merging the dataset
df["Sepal.Width"]=np.array(l1).astype(float)
df["Sepal.Length"]=np.array(l2).astype(float)
df.head()

**Output:**
**Sepal Width & Length	Petal.Width	Species	Sepal.Width	Sepal.Length**
0	3.5,1.4	0.2	setosa	3.5	1.4
1	3,1.4	0.2	setosa	3.0	1.4
2	3.2,1.3	0.2	setosa	3.2	1.3
3	3.1,1.5	0.2	setosa	3.1	1.5
4	3.6,1.4	0.2	setosa	3.6	1.4
```

```python
# Deleting the unwanted column
df.drop("Sepal Width & Length",axis=1)
df.head()

**Output:**
**Sepal Width & Length	Petal.Width	Species	Sepal.Width	Sepal.Length**
0	3.5,1.4	0.2	setosa	3.5	1.4
1	3,1.4	0.2	setosa	3.0	1.4
2	3.2,1.3	0.2	setosa	3.2	1.3
3	3.1,1.5	0.2	setosa	3.1	1.5
4	3.6,1.4	0.2	setosa	3.6	1.4
```

##### Using the Splitting column

```python
# loading the dataset
df2.head()

**Output:**
**Sepal Width & Length	Petal.Width	Species**
0	3.5,1.4	0.2	setosa
1	3,1.4	0.2	setosa
2	3.2,1.3	0.2	setosa
3	3.1,1.5	0.2	setosa
4	3.6,1.4	0.2	setosa
```

```python
# spliting the column
df2[["Sepal.Width", "Sepal.Length"]] = df2["Sepal Width & Length"].str.split(",", expand=True)
df2.head()

**Output:**
**Sepal Width & Length	Petal.Width	Species	Sepal.Width	Sepal.Length**
0	3.5,1.4	0.2	setosa	3.5	1.4
1	3,1.4	0.2	setosa	3.0	1.4
2	3.2,1.3	0.2	setosa	3.2	1.3
3	3.1,1.5	0.2	setosa	3.1	1.5
4	3.6,1.4	0.2	setosa	3.6	1.4
```

#### Data Formatting
Data is usually collected from different places and stored in different formats. bringing data into a common standard of expression allows users to make meaningful comparison.

Sometimes wrong data types will be assigned to a feature.

**Data Types in Pandas**
**1. Numeric Types:**
- **int64:** Represents whole numbers (e.g., 42, -15, 0).
- **float64:** Represents floating-point numbers (e.g., 3.14159, -2.5, 1.0).
**2. String Type:**
- **object:** Represents text data (e.g., "Hello, world!", "pandas").

**3. Boolean Type:**
- **bool:** Represents True or False values.

**4. Datetime Types:**
- **datetime64[ns]:** Represents dates and times with nanosecond precision (e.g., 2023-12-31 23:59:59.999999999).
- **timedelta[ns]:** Represents time differences (e.g., 3 days, 12 hours, 5 minutes).

**5. Categorical Type:**
- **category:** Represents categorical data, which has a limited set of possible values (e.g., colors, genders, countries).

**6. Sparse Dtypes:**
- **SparseDtype:** Represents sparse data, which has many missing values.

**7. Extension Types:**
- pandas can be extended with custom data types to handle specialized data, such as IP addresses or URLs.

**Checking and Converting Data Types:**
- Use `df.dtypes` to check the data types of each column in a Data Frame.
- Use `df['column'].astype(new_type)` to convert a column to a different data type.


#### Data Normalization
Normalization is a technique used to rescale features to a common range, typically between 0 and 1. It's crucial for various machine learning algorithms and statistical analysis scenarios. Here are key reasons to normalize data:

**1. Improving Performance of Machine Learning Algorithms:**
- **Algorithms Sensitive to Feature Scales:** Algorithms like k-Nearest Neighbors, Support Vector Machines, and Neural Networks can be significantly affected by features with varying scales. Normalization ensures equal contribution from all features, preventing those with larger scales from dominating.
- **Efficient Gradient Descent:** Normalization often speeds up gradient descent convergence in algorithms like linear regression and logistic regression, leading to faster training.

**2. Facilitating Comparison of Features:**
- **Understanding Relative Importance:** Normalized features can be directly compared to assess their relative importance within a model, even if they have different original scales.
- **Visualizing Data:** Normalization enables meaningful visualization of data with multiple features, allowing for better interpretation and pattern recognition.

**3. Enhancing Distance-Based Algorithms:**
- **K-Means Clustering:** K-means clustering relies on distance calculations. Normalizing features ensures that distances are not biased by feature scales, leading to more accurate cluster assignments.
- **Dimensionality Reduction:** Techniques like PCA (Principal Component Analysis) benefit from normalization as they're based on covariance matrices, which are sensitive to feature scales.

**4. Reducing the Impact of Outliers:**
- **Mitigating Outlier Influence:** Normalization can reduce the impact of outliers on some algorithms, making them less sensitive to extreme values.

**Specific Scenarios:**
- **Feature Ranges Differ Widely:** When features have significantly different ranges, normalization is essential for fair comparison and algorithm performance.
- **Algorithm Sensitivity to Scale:** Algorithms sensitive to feature scales, as mentioned above, often require normalization.
- **Distance-Based Algorithms:** Algorithms relying on distance calculations, like k-means clustering, often benefit from normalization.
- **Interpretability and Visualization:** Normalization can aid in understanding feature importance and visualizing data with multiple features.

**Considerations:**
- **Not Always Necessary:** Algorithms like decision trees and random forests are generally not affected by feature scales and might not require normalization.
- **Distorted Distributions:** Normalization can distort original distributions, which might be undesirable in certain cases.
- **Alternative Scaling Methods:** Other scaling methods like standardization (transforming features to have a mean of 0 and a standard deviation of 1) might be more suitable in specific situations.

In essence, normalization is a valuable tool to ensure fair comparison of features, improve algorithm performance, enhance distance-based algorithms, and reduce outlier impact. It's essential to consider the specific algorithm and data characteristics to determine whether normalization is appropriate and select the most suitable scaling method.

**Approaches to normalize data**

**Simple Feature Scaling**

$$ x new = x old/xmax $$

```python
# Simple Feature Scaling code
df["col"] = df["col"]/df["col"].max()
```

**Min-Max Method**

$$ x new = (xold-xmin)/(xmax-xmin) $$

```python
# Min-Max Method code
df["col"] = (df["col"] - df["col"].min())/(df["col"].max()- df["col"].min())
```

**Z-score Method**

$$ x new = x old - μ / σ $$

```python
# Z-score Method code
df["col"] = (df["col"] - df["col"].mean())/df["col"].std()
```

### Data Binning

- **Binning**: Grouping of values into bins.
- Converts numeric into categorical variables.
- Group a numeric values into a set of “bins”.

```python
# Binning numeric values
bins = np.linspace(min(df['col']), max(df['col']), 4)

# Grouping names
group_names = ["Low", "Medium", "High"]

# Binning object
df["Col-Binned"] = pd.cut[df["col"], bins, labels=group_names , include_Lowest=True)
```

### **Turning Categorical Variables into Quantitative Variables in Python**

Converting categorical variables into quantitative variables is a crucial step in data preprocessing for machine learning and statistical analysis. pandas offers powerful tools to handle this task effectively. Let's explore two common approaches:

**1. One-Hot Encoding:**

This method creates new dummy variables for each category in the original variable. Each dummy variable takes the value 1 if the observation belongs to that category and 0 otherwise. This essentially transforms the categorical variable into a set of binary features.

Here's how to do it in pandas:

```python
import pandas as pd

# Create a sample DataFrame with a categorical variable
data = {'city': ['London', 'Paris', 'Tokyo', 'London', 'Paris']}
df = pd.DataFrame(data)

# One-hot encode the 'city' variable
df_encoded = pd.get_dummies(df, columns=['city'])

print(df_encoded)

```

This will create three new columns (`city_London`, `city_Paris`, and `city_Tokyo`), each indicating the presence of the corresponding city in that row.

**2. Label Encoding:**

This method assigns unique integer values to each category in the original variable. While simpler than one-hot encoding, it can introduce unwanted ordering between categories, which may not be suitable for all analyses.

Here's how to do it in pandas:

```python
# Label encode the 'city' variable
df['city_encoded'] = df['city'].factorize()[0]

print(df)

```

This will add a new column `city_encoded` with integer codes for each city.


#### Turning Categorical Variables into Quantitative Variables in Python

Converting categorical variables into quantitative variables is a crucial step in data preprocessing for machine learning and statistical analysis. pandas offers powerful tools to handle this task effectively. Let's explore two common approaches:

**1. One-Hot Encoding:**

This method creates new dummy variables for each category in the original variable. Each dummy variable takes the value 1 if the observation belongs to that category and 0 otherwise. This essentially transforms the categorical variable into a set of binary features.

Here's how to do it in pandas:

```python
import pandas as pd

# Create a sample DataFrame with a categorical variable
data = {'city': ['London', 'Paris', 'Tokyo', 'London', 'Paris']}
df = pd.DataFrame(data)

# One-hot encode the 'city' variable
df_encoded = pd.get_dummies(df, columns=['city'])

print(df_encoded)

```

This will create three new columns (`city_London`, `city_Paris`, and `city_Tokyo`), each indicating the presence of the corresponding city in that row.

**2. Label Encoding:**

This method assigns unique integer values to each category in the original variable. While simpler than one-hot encoding, it can introduce unwanted ordering between categories, which may not be suitable for all analyses.

Here's how to do it in pandas:

```python
# Label encode the 'city' variable
df['city_encoded'] = df['city'].factorize()[0]
print(df)
```

This will add a new column `city_encoded` with integer codes for each city.